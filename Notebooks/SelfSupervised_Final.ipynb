{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "gnsZPf0luk0O",
        "zPEVyU5uumAT",
        "sezuezoXumU6",
        "vTwom0pxwetF",
        "ZcVrJxqkwnei",
        "45qyKDqXxFDl",
        "JexctPgOzcZE",
        "yBFe82ZqzwDY",
        "HcaqJseJ0Cj6"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Connect to google drive"
      ],
      "metadata": {
        "id": "gnsZPf0luk0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive/My Drive/[2023-2024] AN2DL/Homework 1"
      ],
      "metadata": {
        "id": "cqxjB1qAunz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries and set parameters"
      ],
      "metadata": {
        "id": "zPEVyU5uumAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix randomness and hide warnings\n",
        "seed = 65\n",
        "\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "os.environ['MPLCONFIGDIR'] = os.getcwd()+'/configs/'\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(seed)\n",
        "\n",
        "import logging\n",
        "\n",
        "import random\n",
        "random.seed(seed)"
      ],
      "metadata": {
        "id": "M-cDMT8nuuyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tfk\n",
        "from tensorflow.keras import layers as tfkl\n",
        "tf.autograph.set_verbosity(0)\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
        "tf.random.set_seed(seed)\n",
        "tf.compat.v1.set_random_seed(seed)\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "JmkDWb47uzFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import other libraries\n",
        "#library for computer vision\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "LlOQ6gVlu2cn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read data and preprocessing"
      ],
      "metadata": {
        "id": "sezuezoXumU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Conditional check for unzipping\n",
        "unzip = False\n",
        "\n",
        "# Unzip the file if the 'unzip' flag is True\n",
        "if unzip:\n",
        "    !unzip public_data.zip\n"
      ],
      "metadata": {
        "id": "DWMegZvuu6HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=np.load('public_data.npz', allow_pickle=True)\n",
        "# get the non normalized data\n",
        "images_not_normalized = data['data']\n",
        "labels_strings= data['labels']\n",
        "# map the labels from string to integers\n",
        "label_map = {\"healthy\": 0, \"unhealthy\": 1}\n",
        "labels = np.vectorize(label_map.get)(labels_strings)\n",
        "print(images_not_normalized.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "zEdXB0yfu6Tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize images\n",
        "images=[]\n",
        "for img in images_not_normalized:\n",
        "  img=(img/255).astype(np.float32)\n",
        "  images.append(img)\n",
        "\n",
        "images= np.array(images)\n",
        "print(img)"
      ],
      "metadata": {
        "id": "iccPJhjGu6kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#DATASET CLEANING!\n",
        "#Remove from the dataset outliers (manually found previously)\n",
        "indices= np.array([ 58, 95, 137, 138, 171, 207, 338,  412, 434, 486, 506, 529, 571, 599, 622, 658, 692, 701, 723, 725, 753, 779, 783, 827, 840, 880, 898, 901, 961, 971, 974, 989,\n",
        " 1028, 1044, 1064, 1065, 1101, 1149, 1172, 1190, 1191, 1265, 1268, 1280, 1333, 1384, 1443, 1466, 1483, 1528, 1541, 1554, 1594, 1609, 1630, 1651, 1690, 1697, 1752, 1757, 1759,\n",
        " 1806, 1828, 1866, 1903, 1938, 1939, 1977, 1981, 1988, 2022, 2081, 2090, 2150, 2191, 2192, 2198, 2261, 2311, 2328, 2348, 2380, 2426, 2435, 2451, 2453, 2487, 2496, 2515, 2564, 2581,\n",
        " 2593, 2596, 2663, 2665, 2676, 2727, 2734, 2736, 2755, 2779, 2796, 2800, 2830, 2831, 2839, 2864, 2866, 2889, 2913, 2929, 2937, 3033, 3049, 3055, 3086, 3105, 3108, 3144, 3155, 3286,\n",
        " 3376, 3410, 3436, 3451, 3488, 3490, 3572, 3583, 3666, 3688, 3700, 3740, 3770, 3800, 3801, 3802, 3806, 3811, 3821, 3835, 3862, 3885, 3896, 3899, 3904, 3927, 3931, 3946, 3950, 3964,\n",
        " 3988, 3989, 4049, 4055, 4097, 4100, 4118, 4144, 4150, 4282, 4310, 4314, 4316, 4368, 4411, 4475, 4476, 4503, 4507, 4557, 4605, 4618, 4694, 4719, 4735, 4740, 4766, 4779, 4837, 4848,\n",
        " 4857, 4860, 4883, 4897, 4903, 4907, 4927, 5048, 5080, 5082, 5121, 5143, 5165, 5171])\n",
        "print(indices.shape)\n",
        "# Define a boolean mask\n",
        "mask = np.ones(len(images), dtype=bool)\n",
        "mask[indices]=False\n",
        "images=images[mask]\n",
        "print(images.shape)"
      ],
      "metadata": {
        "id": "60xBNGefvBQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Bring labels to the correct dimension\n",
        "labels=np.expand_dims(labels,axis=1)\n",
        "labels=labels[mask]\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "avrn8MMvvF4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "labels = tfk.utils.to_categorical(labels,len(np.unique(labels)))"
      ],
      "metadata": {
        "id": "TSLkV550vJRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#split data into training and validation\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, random_state=seed, test_size=.25, stratify=np.argmax(labels,axis=1))"
      ],
      "metadata": {
        "id": "H8hp--CavJkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print shapes of the datasets\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_val shape: {X_val.shape}, y_val shape: {y_val.shape}\")"
      ],
      "metadata": {
        "id": "lsMcK6dWvMid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input shape, output shape, batch size, and number of epochs\n",
        "input_shape = X_train.shape[1:]\n",
        "output_shape = y_train.shape[1:]\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "# Print input shape, batch size, and number of epochs\n",
        "print(f\"Input Shape: {input_shape}, Output Shape: {output_shape}, Batch Size: {batch_size}, Epochs: {epochs}\")"
      ],
      "metadata": {
        "id": "Wpl7uXtWvOg8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <h1>SELF SUPERVISED LEARNING</h1>"
      ],
      "metadata": {
        "id": "BorpOEeQ4_Hq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Firtst attempt - Random rotations"
      ],
      "metadata": {
        "id": "saz-qkbYum1s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a new dataset, assigning new labels based on the rotation applied"
      ],
      "metadata": {
        "id": "vTwom0pxwetF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_0 = X_train.copy() # The starting dataset, not rotated\n",
        "X_train_90 = np.rot90(X_train, axes = (1,2)) #Apply a rotation of 90 degrees\n",
        "X_train_180 = np.rot90(X_train, 2, axes = (1,2)) #Apply a rotation of 180 degrees\n",
        "X_train_270 = np.rot90(X_train, 3, axes = (1,2)) # Apply a rotation of 270 degrees\n",
        "\n",
        "# Assigning pseudo-labels to rotated image datasets\n",
        "y_train_0=np.full((3753), 0)\n",
        "y_train_90=np.full((3753), 1)\n",
        "y_train_180=np.full((3753), 2)\n",
        "y_train_270=np.full((3753), 3)"
      ],
      "metadata": {
        "id": "DezEqu-bwJQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Concatenate all the new images and labeks\n",
        "X_train_rot = np.concatenate((X_train_0, X_train_90, X_train_180, X_train_270), axis=0)\n",
        "y_train_rot = np.concatenate((y_train_0, y_train_90, y_train_180, y_train_270), axis=0)"
      ],
      "metadata": {
        "id": "Ars61KZ2wNN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_0\n",
        "del X_train_90\n",
        "del X_train_180\n",
        "del X_train_270\n",
        "del y_train_0\n",
        "del y_train_90\n",
        "del y_train_180\n",
        "del y_train_270"
      ],
      "metadata": {
        "id": "aAQYYPa1wPC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function will distribute the samples uniformly over dataset\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ],
      "metadata": {
        "id": "9MDXsOrBwRDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly shuffling the concatenated dataset\n",
        "X_train_rot_shuffled, y_train_rot_shuffled = unison_shuffled_copies(X_train_rot, y_train_rot)"
      ],
      "metadata": {
        "id": "a9Ilx-YywYhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_rot\n",
        "del y_train_rot"
      ],
      "metadata": {
        "id": "zPq99-WhwZBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load EffNetB0"
      ],
      "metadata": {
        "id": "ZcVrJxqkwnei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load efficient net b0\n",
        "effnet_b0 = tf.keras.applications.efficientnet.EfficientNetB0(\n",
        "    include_top= False, #discard the dense part\n",
        "    weights= \"imagenet\",\n",
        "    input_shape=(96, 96, 3) ,\n",
        "    pooling= 'max',\n",
        "    )\n",
        "tfk.utils.plot_model(effnet_b0, show_shapes=True)"
      ],
      "metadata": {
        "id": "ysXCpBOgwZRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(seed)\n",
        "\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "\n",
        "#Create a new dense part, custom for the pretext task\n",
        "x = effnet_b0(inputs)\n",
        "x = tfkl.Dense(units=128, kernel_initializer=tfk.initializers.HeUniform(seed=seed))(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation1')(x)\n",
        "x = tfkl.Dropout(rate=0.45, seed=seed)(x)\n",
        "x = tfkl.Dense(units=64, kernel_initializer=tfk.initializers.HeUniform(seed=seed))(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation2')(x)\n",
        "x = tfkl.Dropout(rate=0.45, seed=seed)(x)\n",
        "#4 output neurons, as the types of rotations applies\n",
        "outputs = tfkl.Dense(units=4,activation = 'softmax')(x)\n",
        "\n",
        "effnet_b0_model = tfk.Model(inputs = inputs, outputs=outputs, name='effnet_b0_tl')\n",
        "\n",
        "effnet_b0_model.compile(loss=tfk.losses.SparseCategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4),\\\n",
        "                  metrics=['accuracy'])\n",
        "effnet_b0_model.summary()"
      ],
      "metadata": {
        "id": "k3f4pqhJwZfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Validation and Test Dataset for Pretext Task\n",
        "X_rot_val, X_rot_train = X_train_rot_shuffled[:1000], X_train_rot_shuffled[1000:]\n",
        "y_rot_val, y_rot_train = y_train_rot_shuffled[:1000], y_train_rot_shuffled[1000:]"
      ],
      "metadata": {
        "id": "lpRWvSJBwZsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_rot_shuffled\n",
        "del y_train_rot_shuffled"
      ],
      "metadata": {
        "id": "vUlHrWjLwZ5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, composed of 3 steps:\n",
        "#- Train on the pretext task, with all layers unfrozen\n",
        "#- Train on the downstream task, with only the classifier unfrozen\n",
        "#- Finetune on the downstream task"
      ],
      "metadata": {
        "id": "45qyKDqXxFDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "eff_history = effnet_b0_model.fit(\n",
        "    x = X_rot_train*255, #Effnet expexts input in 0-255 range\n",
        "    y = y_rot_train,\n",
        "    batch_size = 16,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_rot_val*255, y_rot_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=30, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-5, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "qMqdOloTxHIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_model.save(\"EffnetB0_self_supervised_rotations_step1\")"
      ],
      "metadata": {
        "id": "YWNqqh_YxIn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del effnet_b0_model"
      ],
      "metadata": {
        "id": "nW9oq--sxL9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_rot_train\n",
        "del y_rot_train\n",
        "del X_rot_val\n",
        "del y_rot_val"
      ],
      "metadata": {
        "id": "9I6ixDlexMKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft = tfk.models.load_model(\"EffnetB0_self_supervised_rotations_step1\")"
      ],
      "metadata": {
        "id": "MU5NeA_-xMZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft.summary()"
      ],
      "metadata": {
        "id": "n6BR3hNaxMo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft = tf.keras.models.Sequential(effnet_b0_ft.layers[:-7])\n",
        "effnet_b0_ft._name = \"efficientnetb0\"\n",
        "effnet_b0_ft.summary()"
      ],
      "metadata": {
        "id": "QJsnpF8CxQ-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft.trainable = False # Freeze all layers of effnetb0\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "preprocessing = tf.keras.Sequential([ #a layer that applies the augmentaiton\n",
        "    tfkl.RandomFlip(\"vertical\"),\n",
        "    tfkl.RandomFlip(\"horizontal\"),\n",
        "    tfkl.RandomRotation(0.5),\n",
        "], name='preprocessing')\n",
        "\n",
        "\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "preprocessing = preprocessing(inputs)\n",
        "x = effnet_b0_ft(preprocessing)\n",
        "\n",
        "#Create a new dense part, custom for the downstream task, once the network has learned the features with the pretext task\n",
        "x = tfkl.Dense(units=128, kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='HiddenDense1')(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation1')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "x = tfkl.Dropout(rate=0.3, seed=seed)(x)\n",
        "x = tfkl.Dense(units=64, kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='HiddenDense2')(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation2')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "x = tfkl.Dropout(rate=0.3, seed=seed)(x)\n",
        "\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "eff_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "eff_model.compile(loss=tfk.losses.CategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4),\\\n",
        "                  metrics=['accuracy'])\n",
        "eff_model.summary()"
      ],
      "metadata": {
        "id": "6Y9aOYdJxRLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "ft_history = eff_model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 32,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=25, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-5, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "t3kZ6SVxxRaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.save(\"EffnetB0_self_supervised_rotations_step2\")"
      ],
      "metadata": {
        "id": "cj4OFQbaxRpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.get_layer('efficientnetb0').trainable = True\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "9oPfTVT_xg8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze first N layers, e.g., until the 218\n",
        "N = 218\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers[:N]):\n",
        "  layer.trainable=False\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "eff_model.summary()"
      ],
      "metadata": {
        "id": "WX-YZVf3xhPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.compile(loss=tfk.losses.CategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-5,weight_decay=5e-5),\\\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "sKUFAzPYxlcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "ft_history = eff_model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 32,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=25, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-6, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "ACyMLWOLxocv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.save(\"effnetB0_Self_Supervised_rotations_FineTuned\")"
      ],
      "metadata": {
        "id": "jjUTwPZ8xqNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Second attempt - Jigsaw puzzles"
      ],
      "metadata": {
        "id": "sEbXNf4bzNRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create a new dataset, assigning new labels based on the permutation applied to the puzzle pieces"
      ],
      "metadata": {
        "id": "JexctPgOzcZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A function that returns the patches of the image\n",
        "def generate_patches(image):\n",
        "    height, width = image.shape[:2]\n",
        "    h_mid, w_mid = height // 2, width // 2\n",
        "\n",
        "    # Cut the image into 4 patches\n",
        "    patch1 = image[:h_mid, :w_mid]\n",
        "    patch2 = image[:h_mid, w_mid:]\n",
        "    patch3 = image[h_mid:, :w_mid]\n",
        "    patch4 = image[h_mid:, w_mid:]\n",
        "\n",
        "    return [patch1, patch2, patch3, patch4]\n",
        "#A function that fiven the patches, returns all possible permutations, so all the new images\n",
        "def generate_all_combinations(patches):\n",
        "\n",
        "    combinations = []\n",
        "    for i in range(4):\n",
        "        for j in range(4):\n",
        "          for k in range(4):\n",
        "            for z in range(4):\n",
        "              if len(set([i, j, k, z])) == 4:\n",
        "                new_image1=np.vstack((patches[i], patches[j]))\n",
        "                new_image2=np.vstack((patches[k], patches[z]))\n",
        "                new_image = np.hstack((new_image1, new_image2))\n",
        "                combinations.append(new_image)\n",
        "    return combinations\n",
        "\n",
        "#Wrapper function that manages the other 2 functions and generates the labels\n",
        "def generate_new_images(images):\n",
        "    new_images = []\n",
        "    labels = []\n",
        "\n",
        "    for image in images:\n",
        "        patches = generate_patches(image)\n",
        "        combinations = generate_all_combinations(patches)\n",
        "        for index,combination in enumerate(combinations):\n",
        "          new_images.append(combination)\n",
        "          labels.append(index)  # Generate labels\n",
        "\n",
        "    return np.array(new_images), np.array(labels)\n",
        "\n",
        "X_train_puzzle, y_train_puzzle = generate_new_images(X_train)"
      ],
      "metadata": {
        "id": "2u-7gsc1zTpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_puzzle.shape)\n",
        "print(y_train_puzzle.shape)"
      ],
      "metadata": {
        "id": "fu577qvGzUAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_puzzle=np.expand_dims(y_train_puzzle,axis=1)\n",
        "print(y_train_puzzle.shape)"
      ],
      "metadata": {
        "id": "PAPFGhlwzURk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs_to_show=100\n",
        "startToShowFrom=0\n",
        "\n",
        "fig, axes = plt.subplots(10, 10, figsize=(30, 20))\n",
        "\n",
        "# Reshape the axes to a 1D array for easier indexing\n",
        "axes = axes.ravel()\n",
        "\n",
        "# Display the first 100 images with an index\n",
        "for i in range(imgs_to_show):\n",
        "    axes[i].imshow(X_train_puzzle[i+startToShowFrom])\n",
        "    axes[i].set_title(f'class: {y_train_puzzle[i]}')\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Ensure tight layout\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the grid of images with labels\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "jtizSDu5zmkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The function will distribute the samples uniformly over dataset\n",
        "def unison_shuffled_copies(a, b):\n",
        "    assert len(a) == len(b)\n",
        "    p = np.random.permutation(len(a))\n",
        "    return a[p], b[p]"
      ],
      "metadata": {
        "id": "uGznHR1yzm4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly shuffling the concatenated dataset\n",
        "X_train_puzzle_shuffled, y_train_puzzle_shuffled = unison_shuffled_copies(X_train_puzzle, y_train_puzzle)"
      ],
      "metadata": {
        "id": "tkh7TxoNznF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_puzzle\n",
        "del y_train_puzzle"
      ],
      "metadata": {
        "id": "O2zod--gzu1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load EffNetB0"
      ],
      "metadata": {
        "id": "yBFe82ZqzwDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0 = tf.keras.applications.efficientnet.EfficientNetB0(\n",
        "    include_top= False, #Discard the dense part\n",
        "    weights= \"imagenet\",\n",
        "    input_shape=(96, 96, 3) ,\n",
        "    pooling= 'max',\n",
        "    )\n",
        "tfk.utils.plot_model(effnet_b0, show_shapes=True)"
      ],
      "metadata": {
        "id": "12Dyj72NzzCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(seed)\n",
        "\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "\n",
        "x = effnet_b0(inputs)\n",
        "\n",
        "#Create a new classifier part, custom for out pretext task\n",
        "x = tfkl.Dense(units=128, kernel_initializer=tfk.initializers.HeUniform(seed=seed))(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation1')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "x = tfkl.Dense(units=64, kernel_initializer=tfk.initializers.HeUniform(seed=seed))(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation2')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "#Output has 24 neurons, 1 for every possible permutation of the patches\n",
        "outputs = tfkl.Dense(units=24,activation = 'softmax')(x)\n",
        "\n",
        "effnet_b0_model = tfk.Model(inputs = inputs, outputs=outputs, name='effnet_b0')\n",
        "\n",
        "effnet_b0_model.compile(loss=tfk.losses.SparseCategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4),\\\n",
        "                  metrics=['accuracy'])\n",
        "effnet_b0_model.summary()"
      ],
      "metadata": {
        "id": "oS6IrX0ez1qI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create splits\n",
        "X_train_puzzle, X_val_puzzle, y_train_puzzle, y_val_puzzle = train_test_split(X_train_puzzle_shuffled, y_train_puzzle_shuffled, random_state=seed, test_size=.25, stratify=np.argmax(y_train_puzzle_shuffled,axis=1))\n"
      ],
      "metadata": {
        "id": "ommaADCoz3wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_puzzle_shuffled\n",
        "del y_train_puzzle_shuffled"
      ],
      "metadata": {
        "id": "7fL6nppOz4HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training, composed of 3 steps:\n",
        "#- Train on the pretext task, with all layers unfrozen\n",
        "#- Train on the downstream task, with only the classifier unfrozen\n",
        "#- Finetune on the downstream task"
      ],
      "metadata": {
        "id": "HcaqJseJ0Cj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "eff_history = effnet_b0_model.fit(\n",
        "    x = X_train_puzzle*255,\n",
        "    y = y_train_puzzle,\n",
        "    batch_size = 80,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_val_puzzle*255, y_val_puzzle),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=8, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=8, min_lr=1e-5, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "e7pfhZziz4Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_model.save(\"EffnetB0_self_supervised_jigsaw_step1\")"
      ],
      "metadata": {
        "id": "ygUekT8a0Gcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del effnet_b0_model"
      ],
      "metadata": {
        "id": "s_3R8qfc0Jf_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del X_train_puzzle\n",
        "del y_train_puzzle\n",
        "del X_val_puzzle\n",
        "del y_val_puzzle"
      ],
      "metadata": {
        "id": "Rm22dBlU0KIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft = tfk.models.load_model(\"EffnetB0_self_supervised_jigsaw_step1\")"
      ],
      "metadata": {
        "id": "4klPSHxD0KYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft.summary()"
      ],
      "metadata": {
        "id": "XbnVpy-60K8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft = tf.keras.models.Sequential(effnet_b0_ft.layers[:-7])\n",
        "effnet_b0_ft._name = \"efficientnetb0\"\n",
        "effnet_b0_ft.summary()"
      ],
      "metadata": {
        "id": "36r8nED20K2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnet_b0_ft.trainable = False #Freeze all layers\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "preprocessing = tf.keras.Sequential([ #Define a layer to appply augmentation\n",
        "    tfkl.RandomFlip(\"vertical\"),\n",
        "    tfkl.RandomFlip(\"horizontal\"),\n",
        "    tfkl.RandomRotation(0.5),\n",
        "], name='preprocessing')\n",
        "\n",
        "\n",
        "inputs = tfk.Input(shape=(96, 96, 3))\n",
        "preprocessing = preprocessing(inputs)\n",
        "x = effnet_b0_ft(preprocessing)\n",
        "#Create a new classifier, custom for out downstream task\n",
        "x = tfkl.Dense(units=128, kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='HiddenDense1')(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation1')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "x = tfkl.Dense(units=64, kernel_initializer=tfk.initializers.HeUniform(seed=seed), name='HiddenDense2')(x)\n",
        "x = tfkl.Activation('relu', name='HiddenActivation2')(x)\n",
        "x = tfkl.BatchNormalization()(x)\n",
        "\n",
        "# Add a Dense layer with 2 units and softmax activation as the classifier\n",
        "outputs = tfkl.Dense(2, activation='softmax')(x)\n",
        "\n",
        "eff_model = tfk.Model(inputs=inputs, outputs=outputs, name='model')\n",
        "\n",
        "eff_model.compile(loss=tfk.losses.CategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-4,weight_decay=5e-4),\\\n",
        "                  metrics=['accuracy'])\n",
        "eff_model.summary()"
      ],
      "metadata": {
        "id": "OFhl4PJ40KEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "ft_history = eff_model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 32,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=25, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-5, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "yi0Wp71f0Wnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.save(\"EffnetB0_self_supervised_jigsaw_step2\")"
      ],
      "metadata": {
        "id": "xj6fnfFl0aed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.get_layer('efficientnetb0').trainable = True\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers):\n",
        "   print(i, layer.name, layer.trainable)"
      ],
      "metadata": {
        "id": "4sOdAB6j0mBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze first N layers, e.g., until the 218\n",
        "N = 218\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers[:N]):\n",
        "  layer.trainable=False\n",
        "for i, layer in enumerate(eff_model.get_layer('efficientnetb0').layers[0].layers):\n",
        "   print(i, layer.name, layer.trainable)\n",
        "eff_model.summary()"
      ],
      "metadata": {
        "id": "NaGpFCOV0oNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.compile(loss=tfk.losses.CategoricalCrossentropy(),\\\n",
        "                  optimizer=tfk.optimizers.AdamW(1e-5,weight_decay=5e-5),\\\n",
        "                  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KWb8zlqH0qe9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "ft_history = eff_model.fit(\n",
        "    x = X_train*255,\n",
        "    y = y_train,\n",
        "    batch_size = 32,\n",
        "    epochs = 600,\n",
        "    validation_data = (X_val*255, y_val),\n",
        "    callbacks = [tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=25, restore_best_weights=True),\n",
        "                 tfk.callbacks.ReduceLROnPlateau(monitor=\"val_accuracy\", factor=0.1, patience=20, min_lr=1e-6, mode='max')]\n",
        ").history"
      ],
      "metadata": {
        "id": "0SPGOANN0tB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eff_model.save(\"effnetB0_Self_Supervised_jigsaw_FineTuned\")"
      ],
      "metadata": {
        "id": "FTlgX-620u79"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
